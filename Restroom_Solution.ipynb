{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figs/IOAI-Logo.png\" alt=\"IOAI Logo\" width=\"200\" height=\"auto\">\n",
    "\n",
    "[IOAI 2025 (Beijing, China), Individual Contest](https://ioai-official.org/china-2025)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IOAI-official/IOAI-2025/blob/main/Individual-Contest/Restroom/Solution/Restroom_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restroom Icon Matching: Reference Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-04T11:33:19.450019Z",
     "iopub.status.busy": "2022-02-04T11:33:19.448976Z",
     "iopub.status.idle": "2022-02-04T11:33:21.323179Z",
     "shell.execute_reply": "2022-02-04T11:33:21.323659Z",
     "shell.execute_reply.started": "2022-02-04T11:30:05.671311Z"
    },
    "id": "WAP2UubJvK-e",
    "outputId": "7dda341d-6351-4f6f-bab3-305e4818481c",
    "papermill": {
     "duration": 1.897624,
     "end_time": "2022-02-04T11:33:21.323833",
     "exception": false,
     "start_time": "2022-02-04T11:33:19.426209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import os\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "%pip install --quiet git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "train_dir = Path(\"../training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPReID(nn.Module):\n",
    "    def __init__(self, unfreeze_last_n_layers=1):\n",
    "        super(CLIPReID, self).__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.clip_model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        \n",
    "        # freeze all parameters firstly\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # unfreeze the last n transformer blocks\n",
    "        total_blocks = len(self.clip_model.visual.transformer.resblocks)\n",
    "        start_block = total_blocks - unfreeze_last_n_layers\n",
    "        \n",
    "        # unfreeze assigned layers\n",
    "        for i in range(start_block, total_blocks):\n",
    "            for param in self.clip_model.visual.transformer.resblocks[i].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # unfreeze the last layer\n",
    "        for param in self.clip_model.visual.ln_post.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"Return numbers and names of trainable parameters\"\"\"\n",
    "        trainable_params = []\n",
    "        total_params = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                trainable_params.append(name)\n",
    "                total_params += param.numel()\n",
    "        return trainable_params, total_params\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = x.to(dtype=torch.float32, device=self.device)\n",
    "        # for training mode, do not use torch.no_grad()\n",
    "        if self.training:\n",
    "            features = self.clip_model.encode_image(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                features = self.clip_model.encode_image(x)\n",
    "        features = features.to(dtype=torch.float32)\n",
    "        return nn.functional.normalize(features, p=2, dim=1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        feat1 = self.forward_one(x1)\n",
    "        feat2 = self.forward_one(x2)\n",
    "        return feat1, feat2\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropMatchDataset(Dataset):\n",
    "    \"\"\"Step one: match crop and orig\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        \n",
    "        # construct positive sample pairs with crop relationship\n",
    "        self.pairs = []  # [(orig_img, crop_img), ...]\n",
    "        \n",
    "        print(\"train_dir:\", os.listdir(os.path.join(data_dir, 'crop', 'male')))\n",
    "        n = len(os.listdir(os.path.join(data_dir, 'crop', 'male')))\n",
    "        print([(os.path.join(data_dir, 'orig', 'male', f'{i}.png'), os.path.join(data_dir, 'crop', 'male', f'{i}.png')) for i in range(1, n+1)])\n",
    "        self.pairs = [(os.path.join(data_dir, 'orig', 'male', f'{i}.png'), os.path.join(data_dir, 'crop', 'male', f'{i}.png')) for i in range(1, n+1)] + \\\n",
    "                     [(os.path.join(data_dir, 'orig', 'female', f'{i}.png'), os.path.join(data_dir, 'crop', 'female', f'{i}.png')) for i in range(1, n+1)]\n",
    "        print(self.pairs, type(self.pairs))\n",
    "        print(f\"Found {len(self.pairs)} matching pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        orig_path, crop_path = self.pairs[idx]\n",
    "        \n",
    "        orig = Image.open(orig_path).convert('RGB')\n",
    "        crop = Image.open(crop_path).convert('RGB')\n",
    "        \n",
    "        orig = self.preprocess(orig)\n",
    "        crop = self.preprocess(crop)\n",
    "        \n",
    "        return orig, crop\n",
    "    \n",
    "def train_crop_matcher(train_dir, epochs=50, batch_size=32, unfreeze_last_n_layers=6):\n",
    "    dataset = CropMatchDataset(train_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    print(f\"Training set size: {len(dataset)}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # assign number of layers to unfreeze when create model\n",
    "    model = CLIPReID(unfreeze_last_n_layers=unfreeze_last_n_layers).to(device).float()\n",
    "    \n",
    "    # print trainable params info\n",
    "    trainable_params, total_params = model.get_trainable_params()\n",
    "    print(f\"Number of trainable parameters: {total_params}\")\n",
    "    print(\"Trainable Layers:\")\n",
    "    for param_name in trainable_params:\n",
    "        print(f\"- {param_name}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5, betas=(0.5, 0.999))\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "    temp = 0.07\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for orig, crop in pbar:\n",
    "            orig = orig.float().to(device)\n",
    "            crop = crop.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            orig_feat = F.normalize(model.forward_one(orig), dim=1)   # |v| = 1\n",
    "            crop_feat = F.normalize(model.forward_one(crop), dim=1)\n",
    "            \n",
    "            # Info-NCE loss\n",
    "            logits = (orig_feat @ crop_feat.t()) / temp    # (B, B)\n",
    "            labels = torch.arange(orig_feat.size(0), device=device)\n",
    "            loss_i2c = F.cross_entropy(logits, labels)  # orig→crop\n",
    "            loss_c2i = F.cross_entropy(logits.t(), labels)  # crop→orig\n",
    "            loss = 0.5 * (loss_i2c + loss_c2i)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.8f}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.8f}')\n",
    "        if avg_loss < best_loss:\n",
    "            torch.save(model.state_dict(), 'crop_model.pth')\n",
    "            best_loss = avg_loss\n",
    "            print(\"Saved best model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_model = train_crop_matcher(train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderMatchDataset(Dataset):\n",
    "    \"\"\"Step 2: match gender relations\"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        _, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        \n",
    "        # construct positive sample pairs (same ID opposite gender)\n",
    "        self.pairs = []  # [(male_img, female_img), ...]\n",
    "        \n",
    "        n = len(os.listdir(os.path.join(data_dir, 'crop', 'male')))\n",
    "        self.pairs = [(os.path.join(data_dir, 'orig', 'male', f'{i}.png'), os.path.join(data_dir, 'orig', 'female', f'{i}.png')) for i in range(1, n+1)]\n",
    "        \n",
    "        print(f\"Founded {len(self.pairs)} pairs of gender matching images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        male_path, female_path = self.pairs[idx]\n",
    "        \n",
    "        male = Image.open(male_path).convert('RGB')\n",
    "        female = Image.open(female_path).convert('RGB')\n",
    "        \n",
    "        male = self.preprocess(male)\n",
    "        female = self.preprocess(female)\n",
    "        \n",
    "        return male, female\n",
    "\n",
    "def train_gender_matcher(train_dir, epochs=50, batch_size=32, unfreeze_last_n_layers=6):\n",
    "    dataset = GenderMatchDataset(train_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    print(f\"Training set size: {len(dataset)}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # assign number of layers to unfreeze when create model\n",
    "    model = CLIPReID(unfreeze_last_n_layers=unfreeze_last_n_layers).to(device).float()\n",
    "    \n",
    "    # print info of trainable parameters\n",
    "    trainable_params, total_params = model.get_trainable_params()\n",
    "    print(f\"Number of trainable parameters: {total_params}\")\n",
    "    print(\"Trainable layers:\")\n",
    "    for param_name in trainable_params:\n",
    "        print(f\"- {param_name}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5, betas=(0.5, 0.999))\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "    temp = 0.07\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for male, female in pbar:\n",
    "            male = male.float().to(device)\n",
    "            female = female.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            male_feat = F.normalize(model.forward_one(male), dim=1)\n",
    "            female_feat = F.normalize(model.forward_one(female), dim=1)\n",
    "            \n",
    "            # Info-NCE loss\n",
    "            logits = (male_feat @ female_feat.t()) / temp\n",
    "            labels = torch.arange(male_feat.size(0), device=device)\n",
    "            loss_i2f = F.cross_entropy(logits, labels)\n",
    "            loss_f2i = F.cross_entropy(logits.t(), labels)\n",
    "            loss = 0.5 * (loss_i2f + loss_f2i)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.8f}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.8f}')\n",
    "        if avg_loss < best_loss:\n",
    "            torch.save(model.state_dict(), 'gender_model.pth')\n",
    "            best_loss = avg_loss\n",
    "            print(\"Saved best model\")\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model = train_gender_matcher(train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, image_dir):\n",
    "    features = {}\n",
    "    for img_name in tqdm(os.listdir(image_dir)):\n",
    "        if img_name.endswith('.png'):\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = model.preprocess(img)\n",
    "            img = img.unsqueeze(0).float().to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                feat = model.forward_one(img)\n",
    "            features[img_name[:-4]] = feat.cpu().numpy()\n",
    "    return features\n",
    "\n",
    "def match_images(query_dir, gallery_dir, prevent_gallery_reuse, save_path):\n",
    "    # Load two models\n",
    "    crop_model = CLIPReID()\n",
    "    gender_model = CLIPReID()\n",
    "    crop_model.load_state_dict(torch.load('crop_model.pth'))\n",
    "    gender_model.load_state_dict(torch.load('gender_model.pth'))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "    crop_model.to(device).eval()\n",
    "    gender_model.to(device).eval()\n",
    "    \n",
    "    # Step1: find crop relations between query and gallery\n",
    "    print(\"Extract features for finding crop relations...\")\n",
    "    query_features = extract_features(crop_model, query_dir)\n",
    "    gallery_features = extract_features(crop_model, gallery_dir)\n",
    "    \n",
    "    # Store corresponding original images of each cropped query\n",
    "    crop_matches = {}\n",
    "    \n",
    "    for q_name, q_feat in query_features.items():\n",
    "        similarities = []\n",
    "        for g_name, g_feat in gallery_features.items():\n",
    "            sim = torch.nn.functional.cosine_similarity(\n",
    "                torch.from_numpy(q_feat),\n",
    "                torch.from_numpy(g_feat),\n",
    "                dim=1\n",
    "            ).item()\n",
    "            similarities.append((g_name, sim))\n",
    "        \n",
    "        # Find the most similar gallery images (crop relation)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        crop_matches[q_name] = similarities[0][0]\n",
    "    \n",
    "    # Step2: Find gender matching in gallery\n",
    "    print(\"Extract features for gender matching...\")\n",
    "    gallery_gender_features = extract_features(gender_model, gallery_dir)\n",
    "    \n",
    "    n = len(os.listdir(query_dir))\n",
    "    results = np.zeros(n)\n",
    "    matched_galleries = set()  # for tracking matched images\n",
    "    print(\"Matching images...\")\n",
    "    \n",
    "    for q_name, matched_crop in tqdm(crop_matches.items()):\n",
    "        # get features of matched_crop\n",
    "        crop_feat = gallery_gender_features[matched_crop]\n",
    "        \n",
    "        # Find the most similar one in gallary, but exclude crop relation and matched images\n",
    "        similarities = []\n",
    "        for g_name, g_feat in gallery_gender_features.items():\n",
    "            if g_name != matched_crop:\n",
    "                if not prevent_gallery_reuse or g_name not in matched_galleries:\n",
    "                    sim = torch.nn.functional.cosine_similarity(\n",
    "                        torch.from_numpy(crop_feat),\n",
    "                        torch.from_numpy(g_feat),\n",
    "                        dim=1\n",
    "                    ).item()\n",
    "                    similarities.append((g_name, sim))\n",
    "        \n",
    "        # Find the most similar one in the remaining images\n",
    "        if similarities:  # make sure candidates are not empty\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            gender_match = similarities[0][0]\n",
    "            if prevent_gallery_reuse:\n",
    "                matched_galleries.add(gender_match)  # add the matched image into set\n",
    "            # print(q_name, gender_match)\n",
    "            results[int(q_name)-1] = int(gender_match)\n",
    "    \n",
    "    # save the results\n",
    "    np.save(save_path, results)\n",
    "    print(f\"Matched {query_dir} with {gallery_dir}, results saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\".\")\n",
    "OUTPUT_PATH = DATA_PATH / \"Scoring\"\n",
    "\n",
    "match_images(query_dir = DATA_PATH / \"validation_set/query\", gallery_dir = DATA_PATH / \"validation_set/gallery\", prevent_gallery_reuse=True, save_path = OUTPUT_PATH / \"submission_a.npy\")\n",
    "match_images(query_dir = DATA_PATH / \"test_set/query\", gallery_dir = DATA_PATH / \"test_set/gallery\", prevent_gallery_reuse=True, save_path = OUTPUT_PATH / \"submission_b.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "files_to_zip = ['./Scoring/submission_a.npy', './Scoring/submission_b.npy']\n",
    "zip_filename = 'submission.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file, os.path.basename(file))\n",
    "\n",
    "print(f'{zip_filename} is created succefully!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "noai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
